{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPdlcstH11mU"
      },
      "source": [
        "# [Medical Appointment No Shows - Group ID: 29]\n",
        "\n",
        "Group Members:\n",
        "\n",
        "\n",
        "\n",
        "*   Can Zunal 29453\n",
        "*   Ahmet Büyükaksoy 29308\n",
        "*   Berk Ay 29026\n",
        "*   Emre Yaman 28927\n",
        "*   Görkem Topçu 28862\n",
        "*   Yağız Toprak Işık 29174\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1iB4MSX3-6L"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "<font color=\"white\">\n",
        "\n",
        "The goal of the project is to identify possible causes that may affect a patient not showing up for a scheduled medical appointment by using statistical analysis. This will be achieved by analyzing a dataset of 110,527 medical appointments. The target variable is whether the patient showed up for the appointment or not. The end goal is to understand why approximately 30% of patients do not show up for their appointments and to use this information to develop strategies for improving attendance rates by using correlations between the independent variables in the dataset and the target variable.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2omK2BI4KyB"
      },
      "source": [
        "### Problem Definition\n",
        "\n",
        "<font color=\"white\">\n",
        "\n",
        "This will be conducted  through the following steps:\n",
        "\n",
        "1.  Data Cleaning: The raw dataset will be cleaned and processed to make sure that it will be suitable for analysis. This may include fixing errors in the data, handling missing values etc.\n",
        "2. Data Exploration: The cleaned dataset will be explored by summary statistics and visualizations to get a clear comprehension of the data and identify any potential trends or patterns. Correlations among the independent variables and target variable will be analyzed by using appropriate statistical tests.\n",
        "3. Machine Learning Implementation: Machine Learning models will be implemented in order to check correlations found between the indepent variables.\n",
        "4.   Result Interpretation: The conclusions from the statistical analysis will be used to form conclusions regarding the potential factors why patients are missing their medical appointments.\n",
        "\n",
        "</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a2QK9jePe-y"
      },
      "source": [
        "# Importing Libraries\n",
        "\n",
        "We will be using the following libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoA36ZTtkpy6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from os.path import join\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCY8s8LkPvOV"
      },
      "source": [
        "#Importing the Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoTm1tvUmDYZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"./drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6wwwyQkaWmG"
      },
      "outputs": [],
      "source": [
        "fname = \"KaggleV2-May-2016.csv\"\n",
        "path_prefix = './drive/MyDrive/CS210_Project_Data'\n",
        "df = pd.read_csv(join(path_prefix, fname))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpJjwdSzP2qp"
      },
      "source": [
        "# Features in the data set\n",
        "The data consist of 110.527 medical appointments information classified by 14 associated variables namely = \n",
        "\n",
        "{PatientID, AppointmentID, Gender, DataMarcacaoColsulta, DataAgadamento, Age, Neighbourhood, Scholarship, Hipertension, Diabetes, Alcoholism, Handcap, SMS_recieved, No-Show}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USAFoZfeaSTo"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMKewa5rRU63"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GowaJOCYatLq"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U4GM_bHbSYn"
      },
      "source": [
        "**Date cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAUtwKeQmJ1h"
      },
      "source": [
        "<font color=\"white\">\n",
        "In this part, it checks if there is any missing value in each collumn and it shows how many there are.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVPJPl0HmEjU"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MPPBjDkmgrX"
      },
      "source": [
        "Correction of incorrect column names\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fxpn4tFmecE"
      },
      "outputs": [],
      "source": [
        "df.rename(columns = {'Handcap':'Handicap', 'No-show':'NoShow'}, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvdlTTGLqjCk"
      },
      "source": [
        "We convert date data from string to datetime format for making it easier for future use .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYVIS8o8a_Ul"
      },
      "outputs": [],
      "source": [
        "df['ScheduledDay'] = pd.to_datetime(df.ScheduledDay)\n",
        "df['AppointmentDay'] = pd.to_datetime(df.AppointmentDay)\n",
        "\n",
        "# parse datetime to time\n",
        "df['ScheduledTime'] = df.ScheduledDay.dt.time\n",
        "\n",
        "# normalize to set time to midnight, parse datetime to date\n",
        "df['ScheduledDay'] = df.ScheduledDay.dt.normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHiPRtF7av4k"
      },
      "outputs": [],
      "source": [
        "df['WaitingDays'] = df['AppointmentDay'] - df['ScheduledDay']\n",
        "\n",
        "df['WaitingDays'] = df.WaitingDays.dt.days\n",
        "\n",
        "# detect incorrect data\n",
        "df.loc[df['WaitingDays'] < 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ENUD9KQbPeb"
      },
      "outputs": [],
      "source": [
        "# clean data from incorrect data according to scheduledDay and appointmentDay\n",
        "drop_idx = df.loc[df['WaitingDays'] < 0].index\n",
        "df.drop(drop_idx, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy4D1-jFR9pH"
      },
      "source": [
        "## Example Medical Appointments Information "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfMHlSOxR3vi"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ict00io34o_x"
      },
      "source": [
        "# Data Exploration\n",
        "\n",
        "<font color=\"white\">\n",
        "In this part the cleaned dataset will be explored by summary statistics and visualizations to get a clear comprehension of the data and identify any potential trends or patterns. Correlations among the independent variables and target variable will be analyzed by using appropriate statistical tests. Data exploration is divided into 2 parts: Correlations between variables and no show rate and additional correlations. Additional correlations are used in order to better understand the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###  Table Of Contents:\n",
        "* Correlations Between Variables and No Show Rate:\n",
        "1. No-show Rate\n",
        "2. Correlation Between Recieving an SMS and No Show\n",
        "3. No-show and Gender Correlation\n",
        "4. No-show Rate With Different Neighbourhoods\n",
        "5. Probability of showing up depending on the disease and scholarship\n",
        "6. Probability that Someone Shows Up Given That Person Has No Disease\n",
        "7. Probability of showing up based on age\n",
        "8. Probability of Showing Up Depending on the Disease and Scholarship\n",
        "9. Probability of Showing Up Based on Age\n",
        "10. Probability of Showing Up Based on the Day of the Week\n",
        "\n",
        "* Additional Correlations:\n",
        "1. Correlation of Features\n",
        "2. Disease and Age Correlation\n",
        "3. Disease and Gender Correlation\n",
        "\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N95TY5MLXgyx"
      },
      "source": [
        "# Correlations Between Variables and No Show Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm-K7ApRXP2V"
      },
      "source": [
        "##No-show Rate\n",
        "\n",
        "Around 20.2% of people who make appointments don't show up to their appointment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O3P6zlcClfmw"
      },
      "outputs": [],
      "source": [
        "totalShow  = len(df[df.NoShow == \"No\"])\n",
        "totalNoShow = len(df) - totalShow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WlDg6lYXXci8"
      },
      "outputs": [],
      "source": [
        "showDict = {\"Absent\" : totalNoShow, \"Present\" :  totalShow}\n",
        "\n",
        "showDict = dict(sorted(showDict.items()))\n",
        "\n",
        "keys = list(showDict.keys())\n",
        "values = list(showDict.values())\n",
        "\n",
        "plt.pie(values, labels=keys, colors=['pink', 'lightblue'], autopct = \"%1.1f%%\")\n",
        "\n",
        "plt.title(\"Appointment Information: Absent or not?\", fontsize = 16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxaCXtLSwLPd"
      },
      "source": [
        "## Correlation Between Recieving an SMS and No Show\n",
        "\n",
        "From the the graph we can observe that people that recieved SMS tend to show up to their appointments while those that did not receive a SMS are absent. This is a clear indication that SMS help people remember their appointment and increase the rate they show up to the appointments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UitkMYmuwMAa"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(x = df[\"SMS_received\"], hue = df[\"NoShow\"], data=df)\n",
        "ax.set_title(\"Show/NoShow for SMSReceived\")\n",
        "x_ticks_labels=['SMSReceived', 'No SMSReceived']\n",
        "ax.set_xticklabels(x_ticks_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk-37q_LP8pv"
      },
      "source": [
        "##No-show and Gender Correlation\n",
        "In this part, correlation between genders and presence at the appointment are evaluated via bar chart. FEMALE patient to MALE patient ratio is 1.857. Out of 71837 FEMALE patient 57246 of them attended their appointment. Ratio of show is 0.797. Out of 38685 MALE patient 30962 of them attended their appointment. Ratio of show is 0.8. We can say that there is no meaningful difference as 0.797 and 0.8 is extremely close."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1bmcXHNKvWVr"
      },
      "outputs": [],
      "source": [
        "# \"No Show\" = \"No\" means patient attended appointment and \"No Show\" = \"Yes\" means patient missed appointment.\n",
        "female_count = len(df[df[\"Gender\"] == \"F\"])\n",
        "male_count = len(df[df[\"Gender\"] == \"M\"])\n",
        "\n",
        "female_show = 0\n",
        "male_show = 0\n",
        "\n",
        "female_show_ratio = 0\n",
        "male_show_ratio = 0\n",
        "\n",
        "for i in df[df[\"NoShow\"] == \"No\"] [\"Gender\"]:\n",
        "  if i == \"F\":\n",
        "    female_show += 1\n",
        "  elif i == \"M\":\n",
        "    male_show += 1\n",
        "\n",
        "female_show_ratio = round(female_show / female_count,3)\n",
        "male_show_ratio = round(male_show / male_count,3)\n",
        "\n",
        "print(\"FEMALE patient to MALE patient ratio(F/M):\", round(female_count/male_count, 3))\n",
        "print(\"Out of\", female_count, \"FEMALE patient\", female_show, \"of them attended their appointment. Ratio:\", female_show_ratio)\n",
        "print(\"Out of\", male_count, \"MALE patient\", male_show, \"of them attended their appointment. Ratio:\", male_show_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0SSjCaMaWseX"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(x=df[\"Gender\"], hue=df[\"NoShow\"], data=df)\n",
        "ax.set_title(\"Present/Absent for Females and Males\")\n",
        "x_ticks_labels=['Female', 'Male']\n",
        "ax.set_xticklabels(x_ticks_labels)\n",
        "plt.legend(title='Present', loc='upper right', labels=['Yes', 'No'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIGfzmTilCTL"
      },
      "source": [
        "\n",
        "## Noshow Up Rate With Different Neighbourhoods\n",
        "\n",
        "<font color=\"white\">\n",
        "In this part we intend to figure out if there is a correlation between the neighborhoods the patients live in and their show up rate. In order to do this, we created a new pandas dataframe taking neighborhood names as key words, with show rate and total appointment information. A possible scenario is that some of the neighborhoods have poorer conditions thus inability to attend appointments.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_2SxKjcKk83T"
      },
      "outputs": [],
      "source": [
        "neighbourhoodList = {}\n",
        "neighbourhoodShow = {}\n",
        "\n",
        "for row in range(len(df)):\n",
        "  neighbourHood = df.iloc[row][6]\n",
        "  noshow = df.iloc[row][13]\n",
        "\n",
        "  if neighbourHood not in neighbourhoodList: #Add\n",
        "    neighbourhoodList[neighbourHood] = 1\n",
        "\n",
        "    neighbourhoodShow[neighbourHood] = 0\n",
        "    if noshow == \"No\":\n",
        "      neighbourhoodShow[neighbourHood] = 1\n",
        "\n",
        "  else:\n",
        "    neighbourhoodList[neighbourHood] += 1\n",
        "    if noshow == \"No\":\n",
        "      neighbourhoodShow[neighbourHood] += 1\n",
        "\n",
        "# We will work on this later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9VgcXhjlMD4"
      },
      "source": [
        "The mean and of the show rate of all the neighborhoods are respectively 79.46% and 80.24%. Number of neighborhoods that have show up rate average less than the total average is 30 while those who have greater average are 51. Standart deviation is 9.72 which is low, it can be said that the data is clustered around the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "faA091islH_2"
      },
      "outputs": [],
      "source": [
        "df_n = pd.DataFrame(neighbourhoodList.keys(), columns = [\"Neighborhoods\"])\n",
        "df_n[\"Show\"] = df_n[\"Neighborhoods\"].map(neighbourhoodShow)\n",
        "df_n[\"TotalAppointment\"] = df_n[\"Neighborhoods\"].map(neighbourhoodList)\n",
        "\n",
        "df_n[\"ShowPercentage\"] = df_n[\"Show\"] / df_n[\"TotalAppointment\"]\n",
        "\n",
        "lessThanAverage = 0\n",
        "greaterThanAverage  = 0\n",
        "for average in df_n.ShowPercentage:\n",
        "  if average < df_n[\"ShowPercentage\"].mean():\n",
        "    lessThanAverage += 1\n",
        "  else:\n",
        "    greaterThanAverage += 1\n",
        "\n",
        "print(\"There are total of\", len(neighbourhoodList), \"neighborhoods.\")\n",
        "print(\"The mean of the show rate of all the neighborhoods is \", format(df_n[\"ShowPercentage\"].mean() * 100, \"0.2f\"), \"%.\", sep = \"\")\n",
        "print(\"The meadian of the show rate of all the neighborhoods is \", format(df_n[\"ShowPercentage\"].median() * 100, \"0.2f\"), \"%.\", sep = \"\")\n",
        "print(\"The standart deviation of the show rate of all the neighborhoods is \", format(df_n[\"ShowPercentage\"].std() * 100, \"0.2f\"), \"%.\", sep = \"\")\n",
        "print(\"Total neighborhood that have average less than the total average \", lessThanAverage, \".\", sep = \"\")\n",
        "print(\"Total neighborhood that have average greater than the total average \", greaterThanAverage, \".\", sep = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO71laj9lnll"
      },
      "source": [
        "###Graphing the Show Percentage Among Different Neighborhoods\n",
        "\n",
        "In order to understand the data better we can compare the neighboorhoods with different show percentages. To visualise this we seperated lowest show percentage neighboorhods from the highest show percentage neighboorhoods and plot them. After observation show rates seem similiar so this feature might be irrelevant. Further information is needed to classify the neighboorhoods. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "alIthYdblacX"
      },
      "outputs": [],
      "source": [
        "df_mostShow = df_n.loc[df_n[\"ShowPercentage\"] > 0.83]\n",
        "sns.barplot(data=df_mostShow, x=\"Neighborhoods\", y=\"ShowPercentage\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Neighborhood no show correlation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y8ha0mHClcrl"
      },
      "outputs": [],
      "source": [
        "df_leastShow = df_n.loc[df_n[\"ShowPercentage\"] < 0.766]\n",
        "\n",
        "sns.barplot(data=df_leastShow, x=\"Neighborhoods\", y=\"ShowPercentage\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Neighborhood no show correlation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPsGzePhofWG"
      },
      "source": [
        "## Probability of Showing Up Depending on the Diseases and Scholarship\n",
        "\n",
        "For each disease and scholarship, the probability of patients showing up is calculated. It can be observed that there is slight difference between people who have diseases compared to average. However, there are some variables that we shouldn't consider because they are to close to the mean. For instance, people that misses their appointment who are alcoholic tends to have an average no show rate that is almost the same as the overall average. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v4gY5F_ur4sl"
      },
      "outputs": [],
      "source": [
        "def calculateProbabilityDisease(disease):\n",
        "  show_up = list(df[df[disease] == 1] [\"NoShow\"])\n",
        "\n",
        "  showup_ratio = (show_up.count(\"No\") / len(show_up))*100\n",
        "  showup_ratio = round(showup_ratio,2)\n",
        "\n",
        "  print(disease + \": \" + str(showup_ratio) + \"%\")\n",
        "\n",
        "disease_names = [\"Hipertension\", \"Diabetes\", \"Alcoholism\", \"Handicap\", \"Scholarship\"]\n",
        "\n",
        "print(\"The probability that someone shows up given that person has at least: \")\n",
        "for dis in disease_names:\n",
        "  calculateProbabilityDisease(dis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU_vGL_Lo6FN"
      },
      "source": [
        "## Probability that Someone Shows Up Given That Person Has No Disease\n",
        "\n",
        "In this part we are checking people who have any disease rather than  checking specific diseases. If they have any disease they are added. This value is almost the same as the mean which is 79.8. This means that we can't consider this in the future as it is not relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4k1XjrQKr_kJ"
      },
      "outputs": [],
      "source": [
        "noshow_up = list(df[(df.Hipertension == 0) & (df.Diabetes == 0) & (df.Alcoholism == 0) & df.Handicap == 0] [\"NoShow\"])\n",
        "\n",
        "showup_ratio = (noshow_up.count(\"No\") / len(noshow_up))*100\n",
        "\n",
        "print(\"The probability that someone shows up given that person has no disease: \" + str(\"{:.2f}\".format(round(showup_ratio, 2))) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhBXxTMDsJnw"
      },
      "source": [
        "##Probability of Showing Up Based on Age\n",
        "\n",
        "In this part we calculated and plotted ages effect on no show rate. As can be observed there is a gradual increase with age. We can say that there is a correlation. Especially 60+ range have really high show rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2_XQ9VoMsKQT"
      },
      "outputs": [],
      "source": [
        "age_group_labels = [\"[0-18)\",\"[18-30)\",\"[30-45)\",\"[45-60)\",\"[60+]\"]\n",
        "\n",
        "def assign_ageGroup(age):\n",
        "  if 0 <= age < 18:  \n",
        "    return age_group_labels[0] # group A\n",
        "  elif age < 30:\n",
        "    return age_group_labels[1] # group B\n",
        "  elif age < 45:\n",
        "    return age_group_labels[2] # group C\n",
        "  elif age < 60:\n",
        "    return age_group_labels[3] # group D\n",
        "  elif 60 <= 60:\n",
        "    return age_group_labels[4] # group E\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "df[\"AgeGroup\"] = df[\"Age\"].apply(assign_ageGroup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nprrUJ0Z4n5l"
      },
      "outputs": [],
      "source": [
        "from pandas.core.resample import f\n",
        "ageGroup_noShow_dict = {}\n",
        "\n",
        "for i in age_group_labels:\n",
        "  ageGroup_noShow_dict[i] = format(round(len(df[(df.NoShow == \"No\") & (df.AgeGroup == i)]) / len(df[df.AgeGroup == i]), 3))\n",
        "\n",
        "# Sort the dictionary by keys\n",
        "ageGroup_noShow_dict = dict(sorted(ageGroup_noShow_dict.items()))\n",
        "\n",
        "keys = list(ageGroup_noShow_dict.keys())\n",
        "values = list(map(float,list(ageGroup_noShow_dict.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FXVGOqJp7Mqx"
      },
      "outputs": [],
      "source": [
        "for i in ageGroup_noShow_dict.items():\n",
        "  print(f\"{i[0]}: {i[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TamFcr3I14m7"
      },
      "outputs": [],
      "source": [
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "sns.barplot(x=keys, y=values, palette=\"Blues\")\n",
        "\n",
        "# Add a title and labels for the x- and y-axes\n",
        "plt.title(\"Probability of showing up based on age\", fontsize=16)\n",
        "plt.xlabel(\"Age Groups\", fontsize=14)\n",
        "plt.ylabel(\"Probability of showing up\", fontsize=14)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbqXMn90pCa9"
      },
      "source": [
        "## Probability of Showing Up Based on the Day of the Week\n",
        "In this part we made a plot to see how day of the appointment effect the no show rate. We can observe that as it gets closer to friday the rate of no show decreases. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kyDKAkqnsPGo"
      },
      "outputs": [],
      "source": [
        "days = ['ScheduledDay', 'AppointmentDay']\n",
        "for c in days:\n",
        "    df[c] = pd.to_datetime(pd.to_datetime(df[c]).dt.date)\n",
        "\n",
        "df['dayofweek']=df['AppointmentDay'].dt.day_name()\n",
        "\n",
        "Shows = df['NoShow'] == \"No\"\n",
        "NoShows = df['NoShow'] == \"Yes\"\n",
        "\n",
        "xtick = np.array(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "df.dayofweek[NoShows].value_counts().plot.line(label='noshows', figsize=(10, 10), marker='o')\n",
        "df.dayofweek[Shows].value_counts().plot.line(label='shows', figsize=(10, 10), marker='x')\n",
        "plt.xticks(np.arange(0, 7, 1), xtick)\n",
        "\n",
        "plt.title('Appointments in day of the week')\n",
        "plt.xlabel('Days of Week')\n",
        "plt.ylabel('Number of Appointments')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxYBR1spWF4u"
      },
      "source": [
        "#Additional Correlations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjMr97cfkuvk"
      },
      "source": [
        "### Correlation of Features\n",
        "\n",
        "<font color=\"white\">\n",
        "In this section we are using a heat map in order visualize correlations between the given features in the data set. With a quick observation we can assume that diabetes and hipertension are highly corelated as they have a correlation value of 0.43.\n",
        "\n",
        "Feature with highest correlation:\n",
        "\n",
        "1.   Age and Hipertenson 0.5\n",
        "2.   Diabetes and Hipertenson 0.43\n",
        "3.   Diabetes and Age 0.29\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "009enalAksO0"
      },
      "outputs": [],
      "source": [
        "ax = plt.axes()\n",
        "ax.set_title(\"Correlation of Features\", fontsize = 18)\n",
        "sns.set(rc = {\"figure.figsize\" : (10, 10)})\n",
        "\n",
        "dataplot = sns.heatmap(df.corr(), annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMb8JMkeQLxv"
      },
      "source": [
        "## Disease and Age Correlation\n",
        "In this part, correlation between ages and each diseases are evaluated via boxplot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUcMiMMytYGg"
      },
      "outputs": [],
      "source": [
        "ages_hipertension = df[df[\"Hipertension\"] == 1] [\"Age\"]\n",
        "ages_diabetes = df[df[\"Diabetes\"] == 1][\"Age\"]\n",
        "ages_alcoholism = df[df[\"Alcoholism\"] == 1][\"Age\"]\n",
        "ages_handicap = df[df[\"Handicap\"] == 1] [\"Age\"]\n",
        "\n",
        "corrDict = {\"Ages_Hipertension\": ages_hipertension, \"Ages_Diabetes\": ages_diabetes, \"Ages_Alcoholism\": ages_alcoholism, \"Ages_Handicap\": ages_handicap}\n",
        "\n",
        "labels, corr = corrDict.keys(), corrDict.values()\n",
        "\n",
        "plt.boxplot(corr)\n",
        "plt.xticks(range(1,len(labels) + 1), labels)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(12,8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4XqAMKWkkUr"
      },
      "source": [
        "## Disease and Gender Correlation\n",
        "In this part, correlation between genders and each diseases are evaluated via bar chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3EvrSgloxE3"
      },
      "outputs": [],
      "source": [
        "f_count = len(df[df[\"Gender\"] == \"F\"])\n",
        "m_count = len(df)-f_count\n",
        "\n",
        "def gender_disease_corr(disease):\n",
        "  genderList_disease = list(df[df[disease] == 1] [\"Gender\"])\n",
        "  f_d_count = genderList_disease.count(\"F\") # female with disease count\n",
        "  m_d_count = len(genderList_disease)-f_d_count  # male with disease count\n",
        "\n",
        "  f_d_ratio = f_d_count/f_count\n",
        "  m_d_ratio = m_d_count/m_count\n",
        "  \n",
        "  data = np.array([int(f_d_ratio * 100), int(m_d_ratio * 100)])\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(6,8)\n",
        "  plt.bar(x = [\"Female\", \"Male\"], height = data)\n",
        "  plt.title(\"Incidence percentage of \" + disease + \" by gender\")\n",
        "  plt.show()\n",
        "\n",
        "  return f_d_ratio, m_d_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ3TuQgZodLi"
      },
      "outputs": [],
      "source": [
        "gender_disease_corr(\"Hipertension\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8jOEHIGolA2"
      },
      "outputs": [],
      "source": [
        "gender_disease_corr(\"Diabetes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysBC09toldw"
      },
      "outputs": [],
      "source": [
        "gender_disease_corr(\"Alcoholism\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp7N1x3qolP8"
      },
      "outputs": [],
      "source": [
        "gender_disease_corr(\"Handicap\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS1RvYaYCjgn"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWoY8F-v4ZU0"
      },
      "source": [
        "# Machine Learning Models\n",
        "\n",
        "<font color=\"white\">\n",
        "This is the section that you primarily need work on for the final report. Implement at least two machine learning models so that you can compare them.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyDE7dbl13h6"
      },
      "source": [
        "As a result of the observed correlations that are evaluated in the data exploration part it is understood that NoShow is not correlated with some of the columns on the dataset. Therefore, in order to train a machine learning model, some of the columns are excluded from the dataframe. Also, NoShow outputs were given as \"Yes\" and \"No\", they are replaced with 1 and 0. 60 percent of the data is used for training, 20 percent is used for validation and remaining 20 percent is used for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67QXh8uI4c9V"
      },
      "source": [
        "### Implementation\n",
        "\n",
        "<font color=\"white\">\n",
        "Implement and evaluate your models. Perform hyperparameter tunning if necessary. Choose the correct evaluation metrics.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsl4uc1QqQej"
      },
      "outputs": [],
      "source": [
        "data = df.drop(columns=['Gender', 'Neighbourhood', 'Age', 'AgeGroup', 'ScheduledTime', 'ScheduledDay', 'AppointmentDay', 'AppointmentID', 'PatientId', 'dayofweek'])\n",
        "data[\"NoShow\"] = data[\"NoShow\"].replace({\"Yes\": 0, \"No\": 1})\n",
        "\n",
        "X = data.drop(columns=['NoShow'])\n",
        "y = data['NoShow']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfhTXBtr4eJe"
      },
      "source": [
        "We selected two different algorithms for machine learning model generation which are Random Forest Algorithm and Gradient Boosting Algorithm, and previously splitted data is used for training these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oipTAqVkBMRD"
      },
      "outputs": [],
      "source": [
        "#Random Forest\n",
        "model_rf = RandomForestClassifier(random_state=0)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "#Gradient Boosting\n",
        "model_gb = GradientBoostingClassifier(random_state=0)\n",
        "model_gb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKN_RTnc6BOk"
      },
      "source": [
        "In this part, the performances of previously created machine learning models are evaluated. As it can be seen from the resulting graphs, machine learning model that is generated by using Random Forest Algorithm performs better. Performance is calculated with the given formula: (TP+TN)/(TP+TN+FP+FN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-cvc29aCttV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_confusion_matrix(model_rf, X_val, y_val, ax=axs[0], cmap='Blues')\n",
        "axs[0].set_title('Random Forest Confusion Matrix')\n",
        "\n",
        "plot_confusion_matrix(model_gb, X_val, y_val, ax=axs[1], cmap=\"Greens\")\n",
        "axs[1].set_title('Gradient Boosting Confusion Matrix')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHQVsGthBYpH"
      },
      "source": [
        "In this part performances are evaluated by ROC curve and precision-recall curve. In the ROC curve y-axis represents the true positive rate and x-axis represents the false positive rate. Thus a curve that is closer to upperleft corner is more efficient. For the precision-recall curve, precision is calculated by TP/(TP+FP, positive predictions) and recall is calcuated by TP/(total positive instances in the data). Therefore, higher precision and recall means higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD5otK_WGC5h"
      },
      "outputs": [],
      "source": [
        "# Your Code\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, roc_curve, auc, precision_recall_curve,roc_auc_score\n",
        "\n",
        "#   ***     BEGIN: ROC CURVE     ***   #\n",
        "# Make predictions on the validation data using the Random Forest model\n",
        "y_pred_proba_rf = model_rf.predict_proba(X_val)[:, 1]\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_val, y_pred_proba_rf)\n",
        "auc_rf = roc_auc_score(y_val, y_pred_proba_rf)\n",
        "\n",
        "# Make predictions on the validation data using the Gradient Boosting model\n",
        "y_pred_proba_gb = model_gb.predict_proba(X_val)[:, 1]\n",
        "fpr_gb, tpr_gb, thresholds_gb = roc_curve(y_val, y_pred_proba_gb)\n",
        "auc_gb = roc_auc_score(y_val, y_pred_proba_gb)\n",
        "\n",
        "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2, figsize=(12,10))\n",
        "\n",
        "# Random Forest ROC\n",
        "ax1.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = %.2f)' % auc_rf)\n",
        "ax1.plot([0,1],[0,1],'r--')\n",
        "ax1.set_xlabel('FPR')\n",
        "ax1.set_ylabel('TPR')\n",
        "ax1.set_title('Random Forest ROC')\n",
        "ax1.set_xlim(0,1)\n",
        "ax1.set_ylim(0,1)\n",
        "ax1.legend()\n",
        "\n",
        "# Gradient Boosting ROC\n",
        "ax2.plot(fpr_gb, tpr_gb, label='Gradient Boosting (AUC = %.2f)' % auc_gb)\n",
        "ax2.plot([0,1],[0,1],'r--')\n",
        "ax2.set_xlabel('FPR')\n",
        "ax2.set_ylabel('TPR')\n",
        "ax2.set_title('Gradient Boosting ROC')\n",
        "ax2.set_xlim(0,1)\n",
        "ax2.set_ylim(0,1)\n",
        "ax2.legend()\n",
        "#   ***     END: ROC CURVE     ***   #\n",
        "\n",
        "\n",
        "#   ***     BEGIN: PRECISION-RECALL     ***   #\n",
        "# Random Forest\n",
        "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_val, y_pred_proba_rf)\n",
        "\n",
        "# Gradient Boosting\n",
        "precision_gb, recall_gb, thresholds_gb = precision_recall_curve(y_val, y_pred_proba_gb)\n",
        "\n",
        "# Plot RF\n",
        "ax3.plot(recall_rf, precision_rf, label='Random Forest')\n",
        "ax3.set_xlabel('Recall')\n",
        "ax3.set_ylabel('Precision')\n",
        "ax3.set_title('Random Forest Precision-Recall curve')\n",
        "ax3.legend()\n",
        "\n",
        "# Plot GB\n",
        "ax4.plot(recall_gb, precision_gb, label='Gradient Boosting')\n",
        "ax4.set_xlabel('Recall')\n",
        "ax4.set_ylabel('Precision')\n",
        "ax4.set_title('Gradient Boosting Precision-Recall curve')\n",
        "ax4.legend()\n",
        "#   ***     END: PRECISION-RECALL     ***   #\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3rEOuXROWcU"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "\n",
        "<font color=\"white\">\n",
        "As it is explained in the previous part higher precision and higher recall means higher performance, so the area under the precision-recall curve (AUPRC) is an indicator of performance. In this part the best n_estimators and the best max_features values are tested for tuning. For each model, new model variations are created by changing n_estimators and max_features iteratively.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cemcp5apHZI-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "# Your Code\n",
        "n_estimators_vals = [50,100,300,500]\n",
        "max_features_vals = [2,3,5,7]\n",
        "auprc_n_estimators_rf = {}\n",
        "auprc_max_features_rf = {}\n",
        "auprc_n_estimators_gb = {}\n",
        "auprc_max_features_gb = {}\n",
        "\n",
        "# Random Boosting\n",
        "best_auprc_rf = 0\n",
        "best_n_estimators_rf = 0\n",
        "print(\"Random Forest:\\n\")\n",
        "for n_estimators in n_estimators_vals:\n",
        "  model_rf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
        "  model_rf.fit(X_train, y_train)\n",
        "  y_pred = model_rf.predict(X_test)\n",
        "  y_pred_proba = model_rf.predict_proba(X_test)[:,1]\n",
        "  auprc_rf = average_precision_score(y_test, y_pred_proba)\n",
        "  auprc_n_estimators_rf[n_estimators] = auprc_rf\n",
        "  if auprc_rf > best_auprc_rf:\n",
        "    best_n_estimators_rf = n_estimators\n",
        "    best_auprc_rf = auprc_rf\n",
        "  print(f\"n_estimators {n_estimators}, AUPRC score:   {auprc_rf}\")\n",
        "\n",
        "print(\"\\nBest n_estimators for Random Forest =\", best_n_estimators_rf,\"\\n\")\n",
        "\n",
        "best_auprc_rf = 0\n",
        "best_max_features_rf = 0\n",
        "for max_features in max_features_vals:\n",
        "  model_rf = RandomForestClassifier(n_estimators=best_n_estimators_rf, max_features=max_features, random_state=0)\n",
        "  model_rf.fit(X_train, y_train)\n",
        "  y_pred = model_rf.predict(X_test)\n",
        "  y_pred_proba = model_rf.predict_proba(X_test)[:,1]\n",
        "  auprc_rf = average_precision_score(y_test, y_pred_proba)\n",
        "  auprc_max_features_rf[max_features] = auprc_rf\n",
        "  if auprc_rf > best_auprc_rf:\n",
        "    best_max_features_rf = max_features\n",
        "    best_auprc_rf = auprc_rf\n",
        "  print(f\"n_estimators {best_n_estimators_rf} max_features {max_features}, AUPRC score:   {auprc_rf}\")\n",
        "\n",
        "print(\"\\nFor Random Forest, best_n_estimators =\", best_n_estimators_rf, \"and best max_features =\", best_max_features_rf, \"\\n\")\n",
        "\n",
        "# Gradient Boosting\n",
        "best_auprc_gb = 0\n",
        "best_n_estimators_gb = 0\n",
        "print(\"\\nGradient Boosting:\\n\")\n",
        "for n_estimators in n_estimators_vals:\n",
        "  model_gb = GradientBoostingClassifier(n_estimators=n_estimators, random_state=0)\n",
        "  model_gb.fit(X_train, y_train)\n",
        "  y_pred = model_gb.predict(X_test)\n",
        "  y_pred_proba = model_gb.predict_proba(X_test)[:,1]\n",
        "  auprc_gb = average_precision_score(y_test, y_pred_proba)\n",
        "  auprc_n_estimators_gb[n_estimators] = auprc_gb\n",
        "  if auprc_gb > best_auprc_gb:\n",
        "    best_n_estimators_gb = n_estimators\n",
        "    best_auprc_gb = auprc_gb\n",
        "  print(f\"n_estimators {n_estimators}, AUPRC score:   {auprc_gb}\")\n",
        "\n",
        "print(\"\\nBest n_estimators for Gradient Boosting =\", best_n_estimators_gb, \"\\n\")\n",
        "\n",
        "best_auprc_gb = 0\n",
        "best_max_features_gb = 0\n",
        "for max_features in max_features_vals:\n",
        "  model_gb = RandomForestClassifier(n_estimators=best_n_estimators_gb, max_features=max_features, random_state=0)\n",
        "  model_gb.fit(X_train, y_train)\n",
        "  y_pred = model_gb.predict(X_test)\n",
        "  y_pred_proba = model_gb.predict_proba(X_test)[:,1]\n",
        "  auprc_gb = average_precision_score(y_test, y_pred_proba)\n",
        "  auprc_max_features_gb[max_features] = auprc_gb\n",
        "  if auprc_gb > best_auprc_gb:\n",
        "    best_max_features_gb = max_features\n",
        "    best_auprc_gb = auprc_gb\n",
        "  print(f\"n_estimators {best_n_estimators_gb} max_features {max_features}, AUPRC score:   {auprc_gb}\")\n",
        "\n",
        "print(\"\\nFor Gradient Boosting, best_n_estimators =\", best_n_estimators_gb, \"and best max_features =\", best_max_features_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4MtFOqW4jdQ"
      },
      "source": [
        "### Results & Discussion\n",
        "\n",
        "<font color=\"white\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecveBwZQMmpv"
      },
      "source": [
        "In this part, performances of Random Forest Model and Gradient Boosting Models are compared after hyperparameter tuning. As it can be observed from the results their performances are similar, but the performance of Random Forest Model is slightly better. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqJFZ5iGMpJb"
      },
      "outputs": [],
      "source": [
        "# Your Code\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
        "\n",
        "# AUPRC - n_estimators\n",
        "ax1.plot(list(auprc_n_estimators_rf.keys()), list(auprc_n_estimators_rf.values()), '--b', marker=\"o\", label=\"Random Forest\")\n",
        "ax1.plot(list(auprc_n_estimators_gb.keys()), list(auprc_n_estimators_gb.values()), '--r', marker=\"o\", label=\"Gradient Boosting\")\n",
        "ax1.set_yticks( np.around(np.linspace(0.7,1),decimals=2))\n",
        "ax1.set_xticks(n_estimators_vals)\n",
        "ax1.set_title(\"AUPRC - n_estimators\")\n",
        "ax1.set_xlabel(\"n_estimators\")\n",
        "ax1.set_ylabel(\"AUPRC\")\n",
        "ax1.legend()\n",
        "\n",
        "# AUPRC - max_feature\n",
        "ax2.plot(list(auprc_max_features_rf.keys()), list(auprc_max_features_rf.values()), '--b', marker=\"o\", label=\"Random Forest\")\n",
        "ax2.plot(list(auprc_max_features_gb.keys()), list(auprc_max_features_gb.values()), '--r', marker=\"o\", label=\"Gradient Boosting\")\n",
        "ax2.set_yticks( np.around(np.linspace(0.7,1),decimals=2))\n",
        "ax2.set_xticks(max_features_vals)\n",
        "ax2.set_title(\"AUPRC - max_feature\")\n",
        "ax2.set_xlabel(\"max_feature\")\n",
        "ax2.set_ylabel(\"AUPRC\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DNveRyxMsrr"
      },
      "outputs": [],
      "source": [
        "# Your Code\n",
        "x_train_val = np.concatenate((X_train, X_val), axis=0)\n",
        "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "final_model = RandomForestClassifier(random_state = 0, n_estimators=best_n_estimators_rf, max_features=best_max_features_rf)\n",
        "\n",
        "# Train the final model on the combined train and validation data\n",
        "final_model.fit(x_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCILkAfFM3l7"
      },
      "outputs": [],
      "source": [
        "# Your Code\n",
        "y_pred_rf = final_model.predict(X_val)\n",
        "\n",
        "# Plot the confusion matrix for the random forest model\n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(14, 7))\n",
        "plot_confusion_matrix(final_model, x_train_val, y_train_val, ax=ax1, cmap = \"Blues\")\n",
        "ax1.set_title('Random Forest Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8mRHzGc4SH0"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "<font color=\"white\">\n",
        "\n",
        "In the project we cleaned the data from the dataset. Then plotted and analyzed it to make some assumptions. After that we used a machine learning model in order to compare our findings. Our main question was what might be the main cause of the no show in appointments and how we can fix it. Absence rate is 20.2%. From the variables we originaly had, we tried to eliminate the unnecessary variables. While neighbourhood patients live in, gender and alcoholism seems to have low correlation and are pretty similiar to the mean, effect of recieving an SMS about the appointment, age of the patient and the day of the appointment can be clearly seen. Also people who have certain disseases have higher show rate. There are some additional variable that also have minor differences, but we won't be mentioning them. \n",
        "\n",
        "In conclusion we can say that to lower the no show rate, following steps could be used:\n",
        "1.   Make sure that each patient recieves an SMS\n",
        "2.  Spread out the appointments appropriately\n",
        "3.  Figure out why there is a change with age, find a solution\n",
        "\n",
        "First solution can be easily achieved as it is cheap to implement. We would only need a bot that sends appointment information to each patient. Second solution assumes that there is a problem with the current appointment system which should be tested and implemented. Third solution needs further research.\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h0G5RNzuABB"
      },
      "source": [
        "# Future Work\n",
        "\n",
        "<font color=\"white\">\n",
        "\n",
        "Our dataset had 14 features which might not be enough to encompass all the reasons behind the low show rate. Additional information might be needed. Besides that our solutions needs to be tested. New appointment schedule system could be implemented and tested to see if appointment of the days really matters. Figuring out why there is a change with age in no show rate might be a future research topic.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw-zON5bt0pd"
      },
      "source": [
        "# Work Division\n",
        "\n",
        "<font color=\"white\">\n",
        "\n",
        "Each teammate had a specific part in the project. However, as we come across problems everyone helped figuring out and correcting them. Parts are summarized below:\n",
        "\n",
        "Yağız Toprak Işık: Data Cleaning\n",
        "\n",
        "Emre Yaman: Data Cleaning, Introduction \n",
        "\n",
        "Ahmet Büyükaksoy: Data Cleaning and Data Exploration Bug Fixing\n",
        "\n",
        "Berk Ay: Future Generation\n",
        "\n",
        "Görkem Topçu: Data Cleaning, Future Generation, Machine Learning Model\n",
        "\n",
        "Can Zunal: Final Report Structure, Future Generation, Machine Learning Model\n",
        "\n",
        "</font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tPdlcstH11mU",
        "B1iB4MSX3-6L",
        "B2omK2BI4KyB",
        "1a2QK9jePe-y",
        "PCY8s8LkPvOV",
        "JpJjwdSzP2qp",
        "GowaJOCYatLq",
        "Xy4D1-jFR9pH",
        "N95TY5MLXgyx",
        "Hm-K7ApRXP2V",
        "xxaCXtLSwLPd",
        "Uk-37q_LP8pv",
        "WIGfzmTilCTL",
        "uO71laj9lnll",
        "tPsGzePhofWG",
        "KU_vGL_Lo6FN",
        "YhBXxTMDsJnw",
        "bbqXMn90pCa9",
        "uxYBR1spWF4u",
        "IjMr97cfkuvk",
        "tMb8JMkeQLxv",
        "F4XqAMKWkkUr",
        "67QXh8uI4c9V",
        "aw-zON5bt0pd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}